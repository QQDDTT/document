# Dictionary

| 日文 | 中文 | 解析 |
| --- | --- | --- |
| エキスパートシステム | 专家系统 |　特定の専門分野における人間の専門知識や判断プロセスをコンピュータに模倣させ、問題解決を支援する人工知能（AI）システムです。 |
| ディープラーニング（深層学習）　| 深层学习 |　人間の脳の神経回路を模した「ニューラルネットワーク」を多層的に（深く）重ね、大量のデータから複雑な特徴やパターンをAI自身が自動で学習する機械学習の手法です。 |
| 機械学習 | 机械学习 | コンピューターが大量のデータからパターンやルールを自動で学習し、その知識を使って未知のデータを予測・分類・判断する技術で、AI（人工知能）を実現する中核技術の一つです。 |
| 学習 | 学习 | 実際に手作業などで集めたデータを基に、機械学習のアルゴリズムに対して特徴量を抽出できるようにするステップです。 |
| 推論 | 推理 | データを使って学習させた機械学習のアルゴリズムを、学習に使ったデータにはなかった未知のデータに対してアルゴリズムを適用するステップです。 |
| 教師あり学習 | 监督学习 | 正解（ラベル）が付いたデータ（教師データ）を使ってAIモデルを訓練し、未知のデータに対して正しい予測や分類ができるようにする機械学習の手法です。 |
| 教師なし学習 | 无监督学习 | 正解ラベル（教師データ）がないデータから、AIが自らパターンや構造を発見する機械学習手法です。 |
| 強化学習 | 强化学习 | AI（エージェント）が環境との相互作用を通じて試行錯誤を繰り返し、長期的に報酬（スコア）を最大化する最適な行動戦略（方策）を自律的に学習する機械学習手法です。 |
| 過学習　| 过拟合 | 機械学習モデルが訓練データに過剰に適合し、ノイズまで学習してしまい、訓練データでの精度は非常に高いのに未知のデータ（テストデータ）では精度が低下する現象です。 |
| 汎化性能 | 泛化能力 | 機械学習モデルが学習に使ったデータ（訓練データ）だけでなく、それまで見たことのない新しいデータに対しても、どれだけ正確に予測・判断できるかという能力のことです。 |
| 標本化 | 采样 | 連続的に変化するアナログ情報（音や光など）を、一定の時間間隔や空間的区切りで計測・抽出して、離散的なデジタルデータに変換するプロセスです。 |
| 量子化 | 量化 | デジタル信号を非常に正確な形式から、より少ないスペースに変換するプロセスであり、その結果、精度が若干低下します。 |
| MNIST（エムにスト）| MNIST | 画像データセット |
| コンペ | 竞赛 | 「競技会」「競争」「コンテスト」を意味する。 |
| ILSVRC | ILSVRC | ImageNetとよばれる大規模（Large Scale）なデータセットを用いた画像（Visual Recognition）のコンペ（Challenge）でした。 |
| CNN（Convolutional Neural Network） | 卷积神经网络 | 畳み込みニューラルネットワーク |
| ニューラルネットワーク（Neural Network） | 神经网络 | 人間の脳の神経回路を模倣した機械学習モデルで、ノード（人工ニューロン）が層状に接続され、データからパターンを学習・認識・予測する技術です。 |
| SVM（サポートベクターマシン） | 支持向量机 | データを最もよく分離する「超平面（境界線）」を見つける教師あり機械学習アルゴリズムで、分類や回帰に使われ、特に少ないデータで高い精度を出すのが特徴です。 |
| ニューロン | 神经元 | 様々な情報や刺激等を伝達している神経細胞です。 |
| シナプス | 突触 | ニューロンとニューロンを繋ぐ接触部分です。 |
| 重み（weight） | 权重 | 入力されたデータに<br>それぞれ 重み をかけて計算します<br>重みが大きいほど、影響が強い<br>重みが小さいほど、影響が弱い |
| 順伝播 | 顺传播 | ニューラルネットワークにおいて入力層から出力層への方向を順方向として入力変数とパラメータをかけ合わせて予測値を計算することを指します。 |
| 逆伝播 | 逆传播 | ニューラルネットワークの学習において、出力層で生じた「予測値と正解のズレ（誤差）」を、入力層に向かって逆向きに伝えていくアルゴリズムです |
| バイアス（Bias） | 偏置 | バイアスはｘと重みｗで合算された値ｘｗに加えます。 |
| 平均二乗誤差(Mean Squared Error, MSE) | 平均二乘误差 |  |
| 誤差逆伝播法（バックプロパゲーション） | 误差反向传播法 | ニューラルネットワークが学習する際に、出力結果と正解との誤差を出力層から入力層へ逆向きに伝播させ、その誤差を小さくするようにネットワーク内の重みやバイアスを自動で調整するアルゴリズムです。 |
| 定数 | 常量 |  |
| 変数 | 变量 |  |
| 極限 | 极限 | 極端な値を表現するためのツールです。 |
| 微分 | 微分 | 求める値と実現の値の差を考えた誤差関数の最小値を求める勾配降下法など利用します。 |
| ベクトル | 向量 | vector 矢印 |
| 行列（matrix） | 矩阵 | 数や記号や式などを縦と横に矩形状に配列したものである。 |
| 集合 | 集合 |  |
| ベン図（Venn diagram）　| 韦恩图 |  |
| 和集合 | 并集 | A または B |
| 積集合 | 交集 | A かつ B |

---
---

# 数式

## シグモイド 関数
- sigmoid
$$P(a \leqq X < b) = \int_{a}^{b} f(x) dx$$
- 微分
$$\frac{d}{dx} \varsigma_{a}(x) = \frac{d}{dx} \left( \frac{1}{1 + e^{-ax}} \right) = \frac{ae^{-ax}}{(1 + e^{-ax})^{2}}$$
$$\frac{d}{dx} \varsigma_{a}(x) = \varsigma_{a}(x) (1 - \varsigma_{a}(x))$$

## 平均二乗誤差
- Mean Squared Error, MSE 
- 平均二乘误差
$$MSE = \frac{1}{N} \sum_{n=1}^{N} (x_n - k_n)^2 = \frac{1}{N} \| \mathbf{x_n} - \mathbf{k_n} \|^2$$


## アタマール積 
- 阿达马乘积
$$\mathbf{A} \odot \mathbf{B} = [a_{ij} \cdot b_{ij}]$$

## 微分の定義
$$\frac{d}{dx}f(x) = f'(x) = \lim_{\Delta x \to 0} \frac{f(x+\Delta x)-f(x)}{\Delta x}$$

## 連鎖率
- chain rule
- 链式法则
$$\begin{cases} y = f(u) \\ u = g(x) \end{cases} \quad \frac{dy}{dx} = \frac{dy}{du} \times \frac{du}{dx}$$

## 偏微分
- **对 $x$ 的偏导数：**
$$\frac{\partial f}{\partial x} = \lim_{\Delta x \to 0} \frac{f(x+\Delta x, y) - f(x, y)}{\Delta x}$$

- 复合函数偏导（链式法则一般式）
$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial u} \frac{\partial u}{\partial x} + \frac{\partial z}{\partial v} \frac{\partial v}{\partial x}$$


## 内積
$$\mathbf{a} \cdot \mathbf{b} = (a_1, a_2) \cdot \begin{pmatrix} b_1 \\ b_2 \end{pmatrix} = a_1 b_1 + a_2 b_2$$

## L1ノルム
- L1 范数
$$\|\mathbf{x}\|_1 = \sum_{i=1}^{n} |x_i|$$

## L2ノルム
- L2 范数定义
$$\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^{n} x_i^2}$$

## 行列の積 
- 矩阵乘法
$$\begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \begin{pmatrix} b_1 \\ b_2 \end{pmatrix} = \begin{pmatrix} a_{11}b_1 + a_{12}b_2 \\ a_{21}b_1 + a_{22}b_2 \end{pmatrix}$$

## アタマール積
- Hadamard 积
$$A \odot B = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \odot \begin{pmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix} = \begin{pmatrix} a_{11}b_{11} & a_{12}b_{12} \\ a_{21}b_{21} & a_{22}b_{22} \end{pmatrix}$$

## 単位行列
- 单位矩阵
- Identity Matrix
$$E = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$$


## 逆行列
- Determinant
- 逆矩阵公式
- 当 $ad - bc \neq 0$ 时，A 的逆矩阵存在。
$$A^{-1} = \frac{1}{ad - bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$$

## 固有値
- 特征值
- 对于正方形矩阵 A，如果存在非零向量 x 和常数 k，使得下式成立，则 k 为特征值，x 为特征向量：
#### 定义式
$$A\mathbf{x} = k\mathbf{x}$$
#### 特征方程
$$\det(A - kE) = 0$$
#### 2x2 展开式
$$\det \begin{pmatrix} a-k & b \\ c & d-k \end{pmatrix} = 0$$

## サラスの公式
- 三阶行列式（沙路公式）
$$\begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \end{vmatrix} = aei + bfg + cdh - (afh + bdi + ceg)$$

## 補集合 絶対補
$$ \bar{A}$$


## 相対補
- A 中 B 的相对补集
$$A \cap \bar{B}$$


## Permutation
- 排列
$${}_n \mathrm{P}_r = n(n-1)(n-2) \dots$$

## Combination
- 组合
$${}_n C_r = \frac{{}_n P_r}{r!}$$

## 確率密度関数
- 古典概率（Theoretical Probability）
$$P(A) = \frac{n(A)}{n(U)} = \frac{\text{A が起こる場合の数}}{\text{起こり得る全ての場合の数}}$$

## 正規分布
$$P(a \leqq X < b) = \int_{a}^{b} f(x) dx$$

---
---

# Python

```python
# Matplotlib
import matplotlib.pyplot as plt
plt.plot([0, 1, 2, 3], [0, 3, 1, 2])
plt.show
```

```python
# Numpy
import numpy as np
A = np.random.randint(0, 10, (4, 5))
# [[6 7 3 6 5]
#  [2 3 1 0 2]
#  [7 2 8 7 1]
#  [0 5 1 3 4]]

np.array([1, 2, 3])
# array([1, 2, 3])

np.array([1, 7, 3]. [0, 5, 2])
# array([[1, 7 ,3], 
#        [0, 5, 2]])

x = np.array([1, 2])
w = np.array([[3, 4], [5, 6]])
x.dot(w)  # x@w
# array([13, 16])

np.zeros(5, dtype=int)
# array([0, 0, 0, 0, 0])

np.zeros(5)
# array([0., 0., 0., 0., 0.])

np.zeros((3, 5), dtype=int)
# array([[0, 0, 0, 0, 0],
#        [0, 0, 0, 0, 0],
#        [0, 0, 0, 0, 0]])

np.ones((3, 5), dtype=int)
# array([[1, 1, 1, 1, 1],
#        [1, 1, 1, 1, 1],
#        [1, 1, 1, 1, 1]])

np.full((3, 4), 3.14)
# array([3.14, 3.14, 3.14, 3.14],
#       [3.14, 3.14, 3.14, 3.14],
#       [3.14, 3.14, 3.14, 3.14])

np.full((3, 4), 3.14, dtype=int)
# array([3, 3, 3, 3],
#       [3, 3, 3, 3],
#       [3, 3, 3, 3])

np.arange(0, 20, 3)
# array([0, 3, 6, 9, 12, 15, 18])

np.linspace(0, 1, 5)
# array([0.  , 0.25, 0.5 , 0.75, 1.  ])

np.random.random((3, 4))
# array([[0.21082511, 0.34469919, 0.09241394, 0.51181511],
#        [0.30194158, 0.37104436, 0.71313691, 0.57343331],
#        [0.25022096, 0.94463827, 0.64323171, 0.5783137 ]])

np.random.normal(0, 1, (3, 3)) # 正規分布
# array([[ 0.83653979, -1.68384113, -0.25594206],
#        [-0.06730962,  0.83811729, -1.55179533],
#        [-0.48725694, -0.5332726 ,  0.28345037]])

np.random.randint(0, 20, (3, 4))
# array([[ 7, 17, 10,  7],
#        [ 4,  8, 10, 16],
#        [ 7,  4, 17,  5]])

np.eye(4) # 単位行列
# array([[1., 0., 0., 0.],
#        [0., 1., 0., 0.],
#        [0., 0., 1., 0.],
#        [0., 0., 0., 1.]])


```

```python
from sklearn import datasets
digits = datasets.load_digits()
digits['target']
# array([0, 1, 2, ..., 8, 9, 8])

digits['images'].ndim # 次元数
# 3

digits['target'].ndim
# 1

digits['images'].shape # 形
# (1797, 8, 8)

digit['target'].shage
# (1797.)

x1 = np.zeros(6, dtype=np.float16)
# array([0., 0., 0., 0., 0., 0.], dtype=float16)
x2 = np.zeros(3, 4, dtype=np.uint8)
# array([[0, 0, 0, 0],
#       [0, 0, 0, 0],
#       [0, 0, 0, 0]], dtype=uint8)
x3 = np.zeros((2, 4, 5), dtype=np.float32)
# array([[[0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0.]],

#        [[0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0.],
#         [0., 0., 0., 0., 0.]]], dtype=float32)

x1.ndim # 1
x2.ndim # 2
x3.ndim # 3
x1.shape # (6,)
x2.shape # (3, 4)
x3.shape # (2, 4, 5)
x1.size # 6
x2.size # 12
x3.size # 40
```

```python
import numpy as np
x1 = np.random.randint(10, size=(5))
# array([5, 3, 4, 9, 1])

x1[0] # 5
x1[-1] # 1
x1[3] = 7 # [5, 3, 4, 7, 1]

x2 = np.random.randint(10, size=(3, 4))
# array([[0, 9, 3, 4],
#        [4, 5, 7, 3],
#        [3, 9, 4, 7]])

x2[0, 0] = 10
# array([[10,  9,  3,  4],
#        [ 4,  5,  7,  3],
#        [ 3,  9,  4,  7]])

# --------------------------------------------------------

x3 = np.random.randint(10, size=(3, 4))
# array([[9, 8, 6, 4],
#        [3, 4, 3, 5],
#        [3, 7, 5, 7]])

x3_sub1 = x3[:2, :2]
# array([[9, 8],
#        [3, 4]])

x3_sub2 = x3[:, :2]
# array([[9, 8],
#        [3, 4],
#        [3, 7]])

x3_sub3 = x3[1:, 1:3]
# array([[4, 3],
#        [7, 5]])

x3_sub3[1, 1] = 12
x3
# array([[ 9,  8,  6,  4],
#        [ 3,  4,  3,  5],
#        [ 3,  7, 12,  7]])

# --------------------------------------------------------

x4 = np.random.randint(10, size=(3, 5))
# array([[8, 8, 4, 1, 6],
#        [9, 3, 0, 4, 6],
#        [9, 1, 1, 7, 9]])

x4_sub_copy = x4[:2, :3].copy()
# array([[8, 8, 4],
#        [9, 3, 0]])

x4_sub_copy[1, 2] = 11
# array([[ 8,  8,  4],
#        [ 9,  3, 11]])

x4
# array([[8, 8, 4, 1, 6],
#        [9, 3, 0, 4, 6],
#        [9, 1, 1, 7, 9]])

# --------------------------------------------------------
grid = np.arange(1, 10)
# array([1, 2, 3, 4, 5, 6, 7, 8, 9])

grid.reshape(3,3)
# array([[1, 2, 3],
#        [4, 5, 6],
#        [7, 8, 9]])

# --------------------------------------------------------
x = np.array([3, 4])
# array([3, 4])
x.reshape(1, 2)
# array([[3, 4]])
x[np.newaxis]
# array([[3, 4]])
x.reshape(2, 1)
# array([[3],
#        [4]])
x[:, np.newaxis]
# array([[3],
#        [4]])

# --------------------------------------------------------
x = np.array(1, 2)
y = np.array(3, 4)
np.concatenate([x, y])
# array([1, 2, 3, 4])

gride = np.array([[1, 2, 3],
                  [4, 5, 6]])
grid.shape
# (2, 3)
grid2 = np.concatenate([grid, grid], axis=0)
# array([[1, 2, 3],
#        [4, 5, 6],
#        [1, 2, 3],
#        [4, 5, 6]])
grid2.shape
# (4, 3)
grid3 = np.concatenate([grid, grid], axis=1)
# array([[1, 2, 3, 1, 2, 3],
#        [4, 5, 6, 4, 5, 6]])
grid3.shape
# (2, 6)

# --------------------------------------------------------
A = np.random.randint(10, size=(3, 2, 3))
np.concatenate([A, A], axis=0).shape
# (6, 2, 3)
np.concatenate([A, A], axis=1).shape
# (3, 4, 3)
np.concatenate([A, A], axis=2).shape
# (3, 2, 6)
```